{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_hvpy_wdates import calculate_timestamps_for_flare, FlareEvent, extract_datetime\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from hvpy import takeScreenshot, create_layers\n",
    "from typing import List, Tuple\n",
    "\n",
    "data_dir = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7093\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kb_archivid</th>\n",
       "      <th>event_starttime</th>\n",
       "      <th>event_peaktime</th>\n",
       "      <th>event_endtime</th>\n",
       "      <th>fl_goescls</th>\n",
       "      <th>ar_noaanum</th>\n",
       "      <th>event_coord1</th>\n",
       "      <th>event_coord2</th>\n",
       "      <th>event_coord3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ivo://helio-informatics.org/FL_SECstandard_201...</td>\n",
       "      <td>2011-10-01 03:50:00.000</td>\n",
       "      <td>2011-10-01 03:56:00.000</td>\n",
       "      <td>2011-10-01 04:04:00.000</td>\n",
       "      <td>C1.3</td>\n",
       "      <td>11305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ivo://helio-informatics.org/FL_SECstandard_201...</td>\n",
       "      <td>2011-10-01 04:44:00.000</td>\n",
       "      <td>2011-10-01 04:50:00.000</td>\n",
       "      <td>2011-10-01 04:53:00.000</td>\n",
       "      <td>C4.1</td>\n",
       "      <td>11302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ivo://helio-informatics.org/FL_SECstandard_201...</td>\n",
       "      <td>2011-10-01 08:56:00.000</td>\n",
       "      <td>2011-10-01 09:59:00.000</td>\n",
       "      <td>2011-10-01 10:17:00.000</td>\n",
       "      <td>M1.2</td>\n",
       "      <td>11305</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ivo://helio-informatics.org/FL_SECstandard_201...</td>\n",
       "      <td>2011-10-02 00:37:00.000</td>\n",
       "      <td>2011-10-02 00:50:00.000</td>\n",
       "      <td>2011-10-02 00:59:00.000</td>\n",
       "      <td>M3.9</td>\n",
       "      <td>11305</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ivo://helio-informatics.org/FL_SECstandard_201...</td>\n",
       "      <td>2011-10-02 03:10:00.000</td>\n",
       "      <td>2011-10-02 03:49:00.000</td>\n",
       "      <td>2011-10-02 04:13:00.000</td>\n",
       "      <td>C1.5</td>\n",
       "      <td>11302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         kb_archivid          event_starttime  \\\n",
       "0  ivo://helio-informatics.org/FL_SECstandard_201...  2011-10-01 03:50:00.000   \n",
       "1  ivo://helio-informatics.org/FL_SECstandard_201...  2011-10-01 04:44:00.000   \n",
       "2  ivo://helio-informatics.org/FL_SECstandard_201...  2011-10-01 08:56:00.000   \n",
       "3  ivo://helio-informatics.org/FL_SECstandard_201...  2011-10-02 00:37:00.000   \n",
       "4  ivo://helio-informatics.org/FL_SECstandard_201...  2011-10-02 03:10:00.000   \n",
       "\n",
       "            event_peaktime            event_endtime fl_goescls  ar_noaanum  \\\n",
       "0  2011-10-01 03:56:00.000  2011-10-01 04:04:00.000       C1.3       11305   \n",
       "1  2011-10-01 04:50:00.000  2011-10-01 04:53:00.000       C4.1       11302   \n",
       "2  2011-10-01 09:59:00.000  2011-10-01 10:17:00.000       M1.2       11305   \n",
       "3  2011-10-02 00:50:00.000  2011-10-02 00:59:00.000       M3.9       11305   \n",
       "4  2011-10-02 03:49:00.000  2011-10-02 04:13:00.000       C1.5       11302   \n",
       "\n",
       "   event_coord1  event_coord2  event_coord3  \n",
       "0             0             0           NaN  \n",
       "1             0             0           NaN  \n",
       "2             6            10           NaN  \n",
       "3            12             9           NaN  \n",
       "4             0             0           NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flares_path = os.path.join(data_dir, 'flares_GOES.csv')\n",
    "\n",
    "df = pd.read_csv(flares_path)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7093\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FlareEvent:\n",
    "    archive_id: str\n",
    "    start_time: datetime\n",
    "    peak_time: datetime\n",
    "    end_time: datetime\n",
    "    flare_class: str\n",
    "\n",
    "def extract_datetime(input) -> datetime:\n",
    "    return datetime.strptime(input, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "flare_events: List[FlareEvent] = []\n",
    "for index, row in df.iterrows():\n",
    "    # Access row values using row['column_name']\n",
    "    \n",
    "    flare_event = FlareEvent(archive_id=row['kb_archivid'], \n",
    "                             start_time=extract_datetime(row['event_starttime']), \n",
    "                             peak_time=extract_datetime(row['event_peaktime']), \n",
    "                             end_time=extract_datetime(row['event_endtime']), \n",
    "                             flare_class=row['fl_goescls'])\n",
    "    flare_events.append(flare_event)\n",
    "\n",
    "print(len(flare_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7093/7093 [19:05<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "source_id = 19\n",
    "\n",
    "valid_flares: List[Tuple[FlareEvent, List[datetime]]] = []\n",
    "\n",
    "# for flare_event in tqdm(flare_events):\n",
    "#     valid, dates = calculate_timestamps_for_flare(flare_event, source_id=source_id)\n",
    "#     if valid:\n",
    "#         valid_flares.append((flare_event, dates))\n",
    "\n",
    "# Define a function to process each flare event\n",
    "def process_flare(flare_event):\n",
    "    valid, dates = calculate_timestamps_for_flare(flare_event, source_id=source_id, delta_before_start=timedelta(hours=10))\n",
    "    if valid:\n",
    "        return (flare_event, dates)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a ThreadPoolExecutor with the desired number of threads\n",
    "num_threads = 2  # Adjust the number of threads as needed\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    print(executor._max_workers)\n",
    "    # Use the executor to process each flare event in parallel\n",
    "    results = list(tqdm(executor.map(process_flare, flare_events), total=len(flare_events)))\n",
    "\n",
    "# Filter out None values and store the valid flares\n",
    "valid_flares = [result for result in results if result is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "pickle_file = 'valid_flares_10h.pickle'\n",
    "\n",
    "if not os.path.exists(pickle_file):\n",
    "    # Save the data to a pickle file\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(valid_flares, f)\n",
    "else:\n",
    "    print('Loading from pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6171\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FlareEvent:\n",
    "    archive_id: str\n",
    "    start_time: datetime\n",
    "    peak_time: datetime\n",
    "    end_time: datetime\n",
    "    flare_class: str\n",
    "\n",
    "# Load the data from the pickle file\n",
    "pickle_file = 'valid_flares_10h.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    valid_flares = pickle.load(f)\n",
    "\n",
    "# Print the loaded data\n",
    "print(len(valid_flares))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FlareEvent(archive_id='ivo://helio-informatics.org/FL_SECstandard_20120111_192531_20111001035000', start_time=datetime.datetime(2011, 10, 1, 3, 50), peak_time=datetime.datetime(2011, 10, 1, 3, 56), end_time=datetime.datetime(2011, 10, 1, 4, 4), flare_class='C1.3'),\n",
       " [datetime.datetime(2011, 9, 30, 17, 50, 4),\n",
       "  datetime.datetime(2011, 9, 30, 19, 51, 34),\n",
       "  datetime.datetime(2011, 9, 30, 21, 52, 19),\n",
       "  datetime.datetime(2011, 9, 30, 23, 53, 49),\n",
       "  datetime.datetime(2011, 10, 1, 1, 54, 34),\n",
       "  datetime.datetime(2011, 10, 1, 3, 56, 4)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_flares[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440 562\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = flares_path = os.path.join(data_dir, 'flare_data.pkl')\n",
    "flare_df = pd.read_pickle(file_path)\n",
    "flare_df.head()\n",
    "\n",
    "mapped_list = [FlareEvent(archive_id=index, \n",
    "                          start_time=row['beginTime'].to_pydatetime(), \n",
    "                          peak_time=row['peakTime'].to_pydatetime(), \n",
    "                          end_time=row['endTime'].to_pydatetime(), \n",
    "                          flare_class=row['classType'])\n",
    "               for index, row in flare_df.iterrows()]\n",
    "\n",
    "\n",
    "filtered = [i for i in mapped_list if i.flare_class.startswith('M')]\n",
    "print(len(filtered), len(mapped_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid flares\n",
    "\n",
    "The `valid_flares.pickle` file contains all flares with sufficient images & spread between start and finish. Next step is to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 59/37026 [00:20<3:29:33,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from hvpy import createScreenshot, takeScreenshot, create_layers, DataSource\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "source_id = 19\n",
    "w = 1000\n",
    "\n",
    "def download_flare(flare_event: FlareEvent, date: datetime, w: int):\n",
    "    fid = flare_event.archive_id.replace('ivo://helio-informatics.org/', '')\n",
    "    date_str = date.replace(tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H_%M_%SZ\")\n",
    "    folder_path = os.path.join(data_dir, 'flare_images', fid, 'source_19')\n",
    "    filename = os.path.join(folder_path, f\"{date_str}\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(f\"{filename}.png\"):\n",
    "        # print(f\"Downloading {filename}\")\n",
    "        createScreenshot(\n",
    "            date=date,\n",
    "            layers=create_layers([(DataSource.HMI_MAG, 100)]),\n",
    "            eventLabels=False,\n",
    "            watermark=False,\n",
    "            imageScale=1,\n",
    "            x1=-1 * w,\n",
    "            y1=-1 * w,\n",
    "            x2=w,\n",
    "            y2=w,\n",
    "            filename=filename,\n",
    "            overwrite=False\n",
    "        )\n",
    "    else:\n",
    "        # print(f\"Skipping {filename}\")\n",
    "        pass\n",
    "            \n",
    "            \n",
    "def download_flares_parallel(flare_list: List[Tuple[FlareEvent, List[datetime]]], w: int):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for flare_event, dates in flare_list:\n",
    "            for date in dates:\n",
    "                future = executor.submit(download_flare, flare_event, date, w)\n",
    "                futures.append(future)\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in tqdm(futures):\n",
    "            future.result()\n",
    "\n",
    "download_flares_parallel(valid_flares, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlareEvent(archive_id='ivo://helio-informatics.org/FL_SECstandard_20120111_192531_20111001035000', start_time=datetime.datetime(2011, 10, 1, 3, 50), peak_time=datetime.datetime(2011, 10, 1, 3, 56), end_time=datetime.datetime(2011, 10, 1, 4, 4), flare_class='C1.3')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "p = os.path.join(data_dir, 'flare_data.pkl')\n",
    "p2 = os.path.join(data_dir, 'valid_flares_10h.pickle')\n",
    "\n",
    "mylist = pickle.load(open(p2, 'rb'))\n",
    "df = pd.read_pickle(p)\n",
    "# df.head()\n",
    "print(mylist[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
